{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kaggle Rossman Competition\n",
    "\n",
    "This was a kaggle competition to forecast sales at a pharmacy chain/dept store in Europe. It was run back in 2015.\n",
    "\n",
    "Rossmann operates over 3,000 drug stores in 7 European countries. As a data analyst for this store chain, you are tasked with forecasting their daily sales for every store for up to six weeks in advance.\n",
    "\n",
    "You will need to look at various factors influencing the forecast predictions - the primary ones being promotions, competition, school and state holidays, seasonality, and locality.\n",
    "\n",
    "\n",
    "While working through this homework, you will:\n",
    "\n",
    "1. see how to \"grid-search\" when the data is too large to use cross-validation. We will also learn how to use sklearn Pipelines to simplify the work-flow of different tranformation steps like Standardizing, One-hot encoding and Imputing missing values\n",
    "2. understand some aspects of feature engineering that come in with continuous and categorical variables, and see some of the new features in sklearn 0.20\n",
    "3. capture results from validation\n",
    "4. learn what \"categorical \"embeddings\" are and how they can be used to improve the performance of a multi-layer percepton\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load , check and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the 'data' folder and engage in some cleaning. A lot of cleaning of this dataset has already been done for us. Some features have been created. In particular we moved from dates to week-of-year, day-of week, etc. For example the 49th and 50th weeks of the year may have higher sales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data/\"train_clean.csv.gz\", compression='gzip').drop(['index', 'PromoInterval'], axis=1)\n",
    "test_df = pd.read_csv(data/\"test_clean.csv.gz\", compression='gzip').drop(['index', 'PromoInterval'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Events'] = train_df['Events'].fillna('None')\n",
    "test_df['Events'] = test_df['Events'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Year</th>\n",
       "      <th>...</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>AfterPromo</th>\n",
       "      <th>BeforePromo</th>\n",
       "      <th>SchoolHoliday_bw</th>\n",
       "      <th>StateHoliday_bw</th>\n",
       "      <th>Promo_bw</th>\n",
       "      <th>SchoolHoliday_fw</th>\n",
       "      <th>StateHoliday_fw</th>\n",
       "      <th>Promo_fw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo  StateHoliday  \\\n",
       "0      1          5  2015-07-31   5263        555     1      1         False   \n",
       "1      2          5  2015-07-31   6064        625     1      1         False   \n",
       "2      3          5  2015-07-31   8314        821     1      1         False   \n",
       "3      4          5  2015-07-31  13995       1498     1      1         False   \n",
       "4      5          5  2015-07-31   4822        559     1      1         False   \n",
       "\n",
       "   SchoolHoliday  Year  ...  AfterStateHoliday  BeforeStateHoliday  \\\n",
       "0              1  2015  ...                 57                   0   \n",
       "1              1  2015  ...                 67                   0   \n",
       "2              1  2015  ...                 57                   0   \n",
       "3              1  2015  ...                 67                   0   \n",
       "4              1  2015  ...                 57                   0   \n",
       "\n",
       "   AfterPromo  BeforePromo  SchoolHoliday_bw  StateHoliday_bw  Promo_bw  \\\n",
       "0           0            0               5.0              0.0       5.0   \n",
       "1           0            0               5.0              0.0       5.0   \n",
       "2           0            0               5.0              0.0       5.0   \n",
       "3           0            0               5.0              0.0       5.0   \n",
       "4           0            0               5.0              0.0       5.0   \n",
       "\n",
       "   SchoolHoliday_fw  StateHoliday_fw  Promo_fw  \n",
       "0               7.0              0.0       5.0  \n",
       "1               1.0              0.0       1.0  \n",
       "2               5.0              0.0       5.0  \n",
       "3               1.0              0.0       1.0  \n",
       "4               1.0              0.0       1.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 91)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of Sales. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000268A5BB50F0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "train_df.hist('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log-transform the dependent variable because it is long-tailed, i.e., the distribution has many occurrences far from the 'central' part of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resp = np.log(train_df['Sales'].copy())\n",
    "train_df = train_df.drop('Sales', axis=1)\n",
    "# We drop the 'Customers' from train_df and 'Id' from test_df as they're not present in the other dataframe\n",
    "train_df = train_df.drop('Customers', axis=1)\n",
    "test_df = test_df.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some idea about our dataset - what are the different variables used? How do their values look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 89)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>...</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>AfterPromo</th>\n",
       "      <th>BeforePromo</th>\n",
       "      <th>SchoolHoliday_bw</th>\n",
       "      <th>StateHoliday_bw</th>\n",
       "      <th>Promo_bw</th>\n",
       "      <th>SchoolHoliday_fw</th>\n",
       "      <th>StateHoliday_fw</th>\n",
       "      <th>Promo_fw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Open  Promo  StateHoliday  SchoolHoliday  \\\n",
       "0      1          5  2015-07-31     1      1         False              1   \n",
       "1      2          5  2015-07-31     1      1         False              1   \n",
       "2      3          5  2015-07-31     1      1         False              1   \n",
       "3      4          5  2015-07-31     1      1         False              1   \n",
       "4      5          5  2015-07-31     1      1         False              1   \n",
       "\n",
       "   Year  Month  Week  ...  AfterStateHoliday  BeforeStateHoliday  AfterPromo  \\\n",
       "0  2015      7    31  ...                 57                   0           0   \n",
       "1  2015      7    31  ...                 67                   0           0   \n",
       "2  2015      7    31  ...                 57                   0           0   \n",
       "3  2015      7    31  ...                 67                   0           0   \n",
       "4  2015      7    31  ...                 57                   0           0   \n",
       "\n",
       "   BeforePromo  SchoolHoliday_bw  StateHoliday_bw  Promo_bw  SchoolHoliday_fw  \\\n",
       "0            0               5.0              0.0       5.0               7.0   \n",
       "1            0               5.0              0.0       5.0               1.0   \n",
       "2            0               5.0              0.0       5.0               5.0   \n",
       "3            0               5.0              0.0       5.0               1.0   \n",
       "4            0               5.0              0.0       5.0               1.0   \n",
       "\n",
       "   StateHoliday_fw  Promo_fw  \n",
       "0              0.0       5.0  \n",
       "1              0.0       1.0  \n",
       "2              0.0       5.0  \n",
       "3              0.0       1.0  \n",
       "4              0.0       1.0  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((844338, 89), (41088, 89))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test datasets are already given to us sorted by the decreasing order of the 'date' column (the latest date being first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2015-07-31\n",
       "1         2015-07-31\n",
       "2         2015-07-31\n",
       "3         2015-07-31\n",
       "4         2015-07-31\n",
       "5         2015-07-31\n",
       "6         2015-07-31\n",
       "7         2015-07-31\n",
       "8         2015-07-31\n",
       "9         2015-07-31\n",
       "10        2015-07-31\n",
       "11        2015-07-31\n",
       "12        2015-07-31\n",
       "13        2015-07-31\n",
       "14        2015-07-31\n",
       "15        2015-07-31\n",
       "16        2015-07-31\n",
       "17        2015-07-31\n",
       "18        2015-07-31\n",
       "19        2015-07-31\n",
       "20        2015-07-31\n",
       "21        2015-07-31\n",
       "22        2015-07-31\n",
       "23        2015-07-31\n",
       "24        2015-07-31\n",
       "25        2015-07-31\n",
       "26        2015-07-31\n",
       "27        2015-07-31\n",
       "28        2015-07-31\n",
       "29        2015-07-31\n",
       "             ...    \n",
       "844308    2013-01-02\n",
       "844309    2013-01-02\n",
       "844310    2013-01-02\n",
       "844311    2013-01-02\n",
       "844312    2013-01-02\n",
       "844313    2013-01-02\n",
       "844314    2013-01-02\n",
       "844315    2013-01-02\n",
       "844316    2013-01-02\n",
       "844317    2013-01-02\n",
       "844318    2013-01-02\n",
       "844319    2013-01-02\n",
       "844320    2013-01-02\n",
       "844321    2013-01-01\n",
       "844322    2013-01-01\n",
       "844323    2013-01-01\n",
       "844324    2013-01-01\n",
       "844325    2013-01-01\n",
       "844326    2013-01-01\n",
       "844327    2013-01-01\n",
       "844328    2013-01-01\n",
       "844329    2013-01-01\n",
       "844330    2013-01-01\n",
       "844331    2013-01-01\n",
       "844332    2013-01-01\n",
       "844333    2013-01-01\n",
       "844334    2013-01-01\n",
       "844335    2013-01-01\n",
       "844336    2013-01-01\n",
       "844337    2013-01-01\n",
       "Name: Date, Length: 844338, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Date # latest date first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday',\n",
       "       'SchoolHoliday', 'Year', 'Month', 'Week', 'Day', 'Dayofweek',\n",
       "       'Dayofyear', 'Is_month_end', 'Is_month_start', 'Is_quarter_end',\n",
       "       'Is_quarter_start', 'Is_year_end', 'Is_year_start', 'Elapsed',\n",
       "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
       "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
       "       'Promo2SinceWeek', 'Promo2SinceYear', 'State', 'file', 'week', 'trend',\n",
       "       'file_DE', 'week_DE', 'trend_DE', 'Date_DE', 'State_DE', 'Month_DE',\n",
       "       'Day_DE', 'Dayofweek_DE', 'Dayofyear_DE', 'Is_month_end_DE',\n",
       "       'Is_month_start_DE', 'Is_quarter_end_DE', 'Is_quarter_start_DE',\n",
       "       'Is_year_end_DE', 'Is_year_start_DE', 'Elapsed_DE', 'Max_TemperatureC',\n",
       "       'Mean_TemperatureC', 'Min_TemperatureC', 'Dew_PointC', 'MeanDew_PointC',\n",
       "       'Min_DewpointC', 'Max_Humidity', 'Mean_Humidity', 'Min_Humidity',\n",
       "       'Max_Sea_Level_PressurehPa', 'Mean_Sea_Level_PressurehPa',\n",
       "       'Min_Sea_Level_PressurehPa', 'Max_VisibilityKm', 'Mean_VisibilityKm',\n",
       "       'Min_VisibilitykM', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h',\n",
       "       'Max_Gust_SpeedKm_h', 'Precipitationmm', 'CloudCover', 'Events',\n",
       "       'WindDirDegrees', 'StateName', 'CompetitionOpenSince',\n",
       "       'CompetitionDaysOpen', 'CompetitionMonthsOpen', 'Promo2Since',\n",
       "       'Promo2Days', 'Promo2Weeks', 'AfterSchoolHoliday',\n",
       "       'BeforeSchoolHoliday', 'AfterStateHoliday', 'BeforeStateHoliday',\n",
       "       'AfterPromo', 'BeforePromo', 'SchoolHoliday_bw', 'StateHoliday_bw',\n",
       "       'Promo_bw', 'SchoolHoliday_fw', 'StateHoliday_fw', 'Promo_fw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.56845649, 8.71012493, 9.02569612, ..., 8.52416881, 8.40983067,\n",
       "       8.69299353])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resp.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Types of variables and cardinality\n",
    "\n",
    "We make a note of which variables are categorical and which are not. This is a choice. If cardinality(unique elements in a variable) is not too high, binning or categorizing can be beneficial. Often this will be true for integer valued variables.\n",
    ">**Binning** is a way to group a number of more or less continuous values into a smaller number of \"bins\" or \"categories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw', 'Promo', 'SchoolHoliday']\n",
    "\n",
    "cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for missing data and store the column names where this happend in the continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionDistance 2186\n",
      "CloudCover 68056\n"
     ]
    }
   ],
   "source": [
    "nacols=[]\n",
    "for v in cont_vars:\n",
    "    if np.sum(train_df[v].isnull()) > 0:\n",
    "        nacols.append(v)\n",
    "        print(v, np.sum(train_df[v].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at some cardinalities (unique values) of the continuous data: if we have none below 10, we won't engage in binning. \n",
    "Print all continuous variables below and also print the unique elements for those variables for which the cardinality is less than 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "for i in cont_vars:\n",
    "    if(len(train_df[i].unique())<10):\n",
    "        print(i,train_df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>My Answer: Therfore we have no continous variables for which the cardinality is less than 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a similar looksie on the categorical variables. Some of these have many levels. Print all categorical variables below and also print the unique elements for those variables for which the cardinality is less than 50.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store  :  [   1    2    3 ... 1115  876  292]\n",
      "DayOfWeek  :  [5 4 3 2 1 7 6]\n",
      "Year  :  [2015 2014 2013]\n",
      "Month  :  [ 7  6  5  4  3  2  1 12 11 10  9  8]\n",
      "Day  :  [31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8\n",
      "  7  6  5  4  3  2  1]\n",
      "StateHoliday  :  [False  True]\n",
      "CompetitionMonthsOpen  :  [24  3 19  9  0 16 17  7 15 22 11 13  2 23 12  4 10  1 14 20  8 18  6 21\n",
      "  5]\n",
      "Promo2Weeks  :  [ 0 25 17  8 13 24 16  7 12 23 15  6 11 22 14  5 10 21  4  9 20  3 19  2\n",
      " 18  1]\n",
      "StoreType  :  ['c' 'a' 'd' 'b']\n",
      "Assortment  :  ['a' 'c' 'b']\n",
      "CompetitionOpenSinceYear  :  [2008 2007 2006 2009 2015 2013 2014 2000 2011 1900 2010 2005 1999 2003\n",
      " 2012 2004 2002 1961 1995 2001 1990 1994 1998]\n",
      "Promo2SinceYear  :  [1900 2010 2011 2012 2009 2014 2015 2013]\n",
      "State  :  ['HE' 'TH' 'NW' 'BE' 'SN' 'SH' 'HB,NI' 'BY' 'BW' 'RP' 'ST' 'HH']\n",
      "Week  :  [31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8\n",
      "  7  6  5  4  3  2  1 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36\n",
      " 35 34 33 32]\n",
      "Events  :  ['Fog' 'None' 'Rain' 'Rain-Thunderstorm' 'Fog-Rain'\n",
      " 'Rain-Hail-Thunderstorm' 'Fog-Rain-Thunderstorm' 'Thunderstorm'\n",
      " 'Rain-Hail' 'Fog-Thunderstorm' 'Rain-Snow' 'Fog-Rain-Hail-Thunderstorm'\n",
      " 'Snow' 'Rain-Snow-Hail' 'Rain-Snow-Hail-Thunderstorm'\n",
      " 'Rain-Snow-Thunderstorm' 'Fog-Rain-Snow' 'Fog-Snow' 'Snow-Hail'\n",
      " 'Fog-Rain-Snow-Hail' 'Fog-Rain-Hail' 'Fog-Snow-Hail']\n",
      "Promo_fw  :  [5. 1. 2. 3. 4. 0.]\n",
      "Promo_bw  :  [5. 4. 3. 2. 1. 0.]\n",
      "StateHoliday_fw  :  [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "StateHoliday_bw  :  [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "SchoolHoliday_fw  :  [7. 1. 5. 4. 2. 3. 0. 6.]\n",
      "SchoolHoliday_bw  :  [5. 7. 0. 2. 4. 1. 3. 6.]\n",
      "Promo  :  [1 0]\n",
      "SchoolHoliday  :  [1 0]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for i in cat_vars:\n",
    "    print(i,\" : \",train_df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a validation set\n",
    "\n",
    "The construction of a validation or \"development\" set is not always a `test_train_split` deal. Here we create a validation set of \"latest\" data, corresponding in date and size to what we have in the test set. Hopefully this will make sure we have similar distributions of features and outcomes on both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the train & test data and answer why we shouldn't have a random test-train split in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MY ANSWER: The Importance of latest data will be more than the old data as it will show the current trend of customers whereas the old data depicts the trend of that time and is not that reliable as the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40282    2015-06-19\n",
       "40283    2015-06-19\n",
       "40284    2015-06-19\n",
       "40285    2015-06-19\n",
       "40286    2015-06-19\n",
       "40287    2015-06-19\n",
       "40288    2015-06-19\n",
       "40289    2015-06-19\n",
       "40290    2015-06-19\n",
       "40291    2015-06-19\n",
       "40292    2015-06-19\n",
       "40293    2015-06-19\n",
       "40294    2015-06-19\n",
       "40295    2015-06-19\n",
       "40296    2015-06-19\n",
       "40297    2015-06-19\n",
       "40298    2015-06-19\n",
       "40299    2015-06-19\n",
       "40300    2015-06-19\n",
       "40301    2015-06-19\n",
       "40302    2015-06-19\n",
       "40303    2015-06-19\n",
       "40304    2015-06-19\n",
       "40305    2015-06-19\n",
       "40306    2015-06-19\n",
       "40307    2015-06-19\n",
       "40308    2015-06-19\n",
       "40309    2015-06-19\n",
       "40310    2015-06-19\n",
       "40311    2015-06-19\n",
       "            ...    \n",
       "41366    2015-06-19\n",
       "41367    2015-06-19\n",
       "41368    2015-06-19\n",
       "41369    2015-06-19\n",
       "41370    2015-06-19\n",
       "41371    2015-06-19\n",
       "41372    2015-06-19\n",
       "41373    2015-06-19\n",
       "41374    2015-06-19\n",
       "41375    2015-06-19\n",
       "41376    2015-06-19\n",
       "41377    2015-06-19\n",
       "41378    2015-06-19\n",
       "41379    2015-06-19\n",
       "41380    2015-06-19\n",
       "41381    2015-06-19\n",
       "41382    2015-06-19\n",
       "41383    2015-06-19\n",
       "41384    2015-06-19\n",
       "41385    2015-06-19\n",
       "41386    2015-06-19\n",
       "41387    2015-06-19\n",
       "41388    2015-06-19\n",
       "41389    2015-06-19\n",
       "41390    2015-06-19\n",
       "41391    2015-06-19\n",
       "41392    2015-06-19\n",
       "41393    2015-06-19\n",
       "41394    2015-06-19\n",
       "41395    2015-06-19\n",
       "Name: Date, Length: 1114, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41395"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-06-19'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Date'][len(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41395"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = range(cut)\n",
    "train_idx = list(np.setdiff1d(range(train_df.shape[0]), valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf = train_df.iloc[train_idx]\n",
    "vadf = train_df.iloc[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943, 89), (41395, 89))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf.shape, vadf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Transformation Pipelines\n",
    "\n",
    "This is the definition of `pipeline` class according to scikit-learn is:\n",
    ">Sequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit.\n",
    "\n",
    "In most ML projects, there are quite often a number of transformational steps such as encoding categorical variables, feature scaling and normalisation that need to be performed. However, in a typical machine learning workflow you will need to apply all these transformations at least twice. Scikit-learn pipelines are a tool to simplify this process.\n",
    "\n",
    "The pipeline class allows sticking multiple processes into a single scikit-learn estimator. It has fit, predict and score method just like any other estimator (ex. LinearRegression).\n",
    "\n",
    "#### Why Pipelines?\n",
    "\n",
    "1. Pipelines enforce implementation and desired order of steps in your project, which in turn helps in reproducibility and creating a convenient work-flow.\n",
    "2. They also prevent data leakage in your validation set during cross-validation by ensuring that data preparation like standardization is constrained to each fold of your cross validation procedure.\n",
    "\n",
    "Ok, now we'll use the new `ColumnTransformer` with imputation, missing-data indicators, the new `OrdinalEncoder`, and the usual `StandardScaler`. The [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) writes this about `ColumnTransformer`:\n",
    ">Applies transformers to columns of an array or pandas DataFrame.\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer,MissingIndicator\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "impu = SimpleImputer(strategy=\"median\") # replaces the missing values of a column by the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = MissingIndicator() # create, fit, and transform a missingness indicator\n",
    "mi.fit(trdf[nacols])\n",
    "Xtrmi = mi.transform(trdf[nacols])\n",
    "Xvami = mi.transform(vadf[nacols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "ss = StandardScaler()\n",
    "oe = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf_cat = trdf[cat_vars]\n",
    "trdf_cont = trdf[cont_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct two pipelines, one for categoricals and one for continuous variables\n",
    "\n",
    "The first step in building the pipeline is to define each transformer type. The convention here is generally to create transformers for the different variable types. All the continuous variables need to be median imputed (wherever required) and scaled, so we create a transformer which includes a SimpleImputer to fill in any missing values and applies a StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_pipe = Pipeline([(\"imp\",impu), (\"scale\", ss)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline `cat_pipe` for categorical variables that should contain a transformer that can one-hot encode each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe= OneHotEncoder(handle_unknown='ignore')\n",
    "cat_pipe=Pipeline([(\"oe\",oe)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine them here in a transformer list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [('cat', cat_pipe, cat_vars),\n",
    "                    ('cont', cont_pipe, cont_vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a `ColumnTransformer` to combine these and fit it on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "ct= ColumnTransformer(transformers=transformers, remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('cat',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('oe',\n",
       "                                                  OrdinalEncoder(categories='auto',\n",
       "                                                                 dtype=<class 'numpy.float64'>))],\n",
       "                                          verbose=False),\n",
       "                                 ['Store', 'DayOfWeek', 'Year', 'Month', 'Day',\n",
       "                                  'StateHoliday', 'CompetitionMonthsOpen',\n",
       "                                  'Promo2Weeks', 'StoreType', 'Assortment',\n",
       "                                  'C...\n",
       "                                                                verbose=0)),\n",
       "                                                 ('scale',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True))],\n",
       "                                          verbose=False),\n",
       "                                 ['CompetitionDistance', 'Max_TemperatureC',\n",
       "                                  'Mean_TemperatureC', 'Min_TemperatureC',\n",
       "                                  'Max_Humidity', 'Mean_Humidity',\n",
       "                                  'Min_Humidity', 'Max_Wind_SpeedKm_h',\n",
       "                                  'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend',\n",
       "                                  'trend_DE', 'AfterStateHoliday',\n",
       "                                  'BeforeStateHoliday'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit(trdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = ct.transform(trdf)\n",
    "Xval = ct.transform(vadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943, 37), (802943, 2), (41395, 37), (41395, 2))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xtrmi.shape,Xval.shape,Xvami.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.11400000e+03  4.00000000e+00  2.00000000e+00 ...  5.96857197e-01\n",
      "  -4.04391639e-01  8.95196255e-01]\n",
      " [ 0.00000000e+00  3.00000000e+00  2.00000000e+00 ...  5.96857197e-01\n",
      "  -4.38484396e-01  8.95196255e-01]\n",
      " [ 1.00000000e+00  3.00000000e+00  2.00000000e+00 ...  5.96857197e-01\n",
      "  -9.75568306e-02  8.95196255e-01]\n",
      " ...\n",
      " [ 7.68000000e+02  1.00000000e+00  0.00000000e+00 ... -3.75933276e-01\n",
      "  -9.15782986e-01  8.95196255e-01]\n",
      " [ 9.47000000e+02  1.00000000e+00  0.00000000e+00 ... -3.75933276e-01\n",
      "  -9.15782986e-01  8.95196255e-01]\n",
      " [ 1.09600000e+03  1.00000000e+00  0.00000000e+00 ... -3.75933276e-01\n",
      "  -9.15782986e-01  8.95196255e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the old indicators back in. The transformer lists all the categoricals first, since thats the first item in `transformers`, so we pre-pend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802943, 39)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = np.concatenate([Xtrmi, Xtr], axis=1)\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41395, 39)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid = np.concatenate([Xvami, Xval], axis=1)\n",
    "Xvalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn-pipelines lose our nice pandas names. so we bring them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, ('CompetitionDistance_missing', 'cont')),\n",
       "  (1, ('CloudCover_missing', 'cont')),\n",
       "  (2, ('Store', 'cat')),\n",
       "  (3, ('DayOfWeek', 'cat')),\n",
       "  (4, ('Year', 'cat')),\n",
       "  (5, ('Month', 'cat')),\n",
       "  (6, ('Day', 'cat')),\n",
       "  (7, ('StateHoliday', 'cat')),\n",
       "  (8, ('CompetitionMonthsOpen', 'cat')),\n",
       "  (9, ('Promo2Weeks', 'cat')),\n",
       "  (10, ('StoreType', 'cat')),\n",
       "  (11, ('Assortment', 'cat')),\n",
       "  (12, ('CompetitionOpenSinceYear', 'cat')),\n",
       "  (13, ('Promo2SinceYear', 'cat')),\n",
       "  (14, ('State', 'cat')),\n",
       "  (15, ('Week', 'cat')),\n",
       "  (16, ('Events', 'cat')),\n",
       "  (17, ('Promo_fw', 'cat')),\n",
       "  (18, ('Promo_bw', 'cat')),\n",
       "  (19, ('StateHoliday_fw', 'cat')),\n",
       "  (20, ('StateHoliday_bw', 'cat')),\n",
       "  (21, ('SchoolHoliday_fw', 'cat')),\n",
       "  (22, ('SchoolHoliday_bw', 'cat')),\n",
       "  (23, ('Promo', 'cat')),\n",
       "  (24, ('SchoolHoliday', 'cat')),\n",
       "  (25, ('CompetitionDistance', 'cont')),\n",
       "  (26, ('Max_TemperatureC', 'cont')),\n",
       "  (27, ('Mean_TemperatureC', 'cont')),\n",
       "  (28, ('Min_TemperatureC', 'cont')),\n",
       "  (29, ('Max_Humidity', 'cont')),\n",
       "  (30, ('Mean_Humidity', 'cont')),\n",
       "  (31, ('Min_Humidity', 'cont')),\n",
       "  (32, ('Max_Wind_SpeedKm_h', 'cont')),\n",
       "  (33, ('Mean_Wind_SpeedKm_h', 'cont')),\n",
       "  (34, ('CloudCover', 'cont')),\n",
       "  (35, ('trend', 'cont')),\n",
       "  (36, ('trend_DE', 'cont')),\n",
       "  (37, ('AfterStateHoliday', 'cont')),\n",
       "  (38, ('BeforeStateHoliday', 'cont'))],\n",
       " 39)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = trdf.columns\n",
    "actcols = []\n",
    "actcolcount = 0\n",
    "nacols_cat = []\n",
    "for k in nacols:\n",
    "    actcols.append((k+'_missing', 'cont'))\n",
    "    nacols_cat.append(k+'_missing')\n",
    "    actcolcount+=1\n",
    "for k in cat_vars+cont_vars:\n",
    "    if k in cat_vars:\n",
    "        actcols.append((k, \"cat\"))\n",
    "        actcolcount+=1\n",
    "    if k in cont_vars:\n",
    "        actcols.append((k, \"cont\"))\n",
    "        actcolcount+=1\n",
    "        \n",
    "list(enumerate(actcols)), actcolcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to learn\n",
    "We will follow two approches here to forecast sales and see how they perform relative to each other:\n",
    "1. Gradient Boosting Regression Trees\n",
    "2. Multi-layer Perceptron Model using Entity Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2. Forecast using Gradient Boosting Regression Trees(GBRT)\n",
    "We need to first split the y (the log of the y, really) into **ytrain** and **yvalid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ytrain=train_resp.iloc[train_idx]\n",
    "yvalid=train_resp.iloc[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943,), (41395,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape, yvalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and import what we need to for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peter Prettenhofer, who wrote sklearns GBRT implementation writes in his pydata14 talk (worth watching!) ([link](https://www.youtube.com/watch?v=-5l3g91NZfQ) here)\n",
    "\n",
    ">Hyperparameter tuning - I usually follow this recipe to tune the hyperparameters:\n",
    "\n",
    "> \n",
    "- Pick n_estimators as large as (computationally) possible (e.g. 3000)\n",
    "- Tune max_depth, learning_rate, min_samples_leaf, and max_features via grid search\n",
    "- A lower learning_rate requires a higher number of n_estimators. Thus increase n_estimators even more and tune learning_rate again holding the other parameters fixed\n",
    "\n",
    ">This last point is a trade-off between number of iterations or runtime against accuracy. And keep in mind that it might lead to overfitting.\n",
    "\n",
    "Let me add however, that poor learners do rather well. So you might want to not cross-validate max_depth. And min_samples_per_leaf is not independent either, so if you do use cross-val, you might just use one of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use Grid-search without cross-validation\n",
    "We use `ParameterGrid` here to construct the entire grid for us! We put the output in a list of dictionaries and then save it in a dataframe. We might want to persist such dataframes to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.1, 0.01],\n",
    "              'max_depth': [1,2, 3],\n",
    "              'max_features': [0.2, 0.6]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the Gradient Boosting Regressor method **(n_estimators = 200)** to fit on your training data. Calculate the mean squared error for both the validation **('mse')** and the training set **('msetr')** for each combination of parameters in the `param_grid` and store the results along with the parameters in a list **'ds'**. [sklearn documentation link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802943, 37)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 1, 'max_features': 0.2}\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'max_features': 0.6}\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'max_features': 0.2}\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'max_features': 0.6}\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 0.2}\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 0.6}\n",
      "{'learning_rate': 0.01, 'max_depth': 1, 'max_features': 0.2}\n",
      "{'learning_rate': 0.01, 'max_depth': 1, 'max_features': 0.6}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 0.2}\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 0.6}\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.2}\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.6}\n"
     ]
    }
   ],
   "source": [
    "ds=[]\n",
    "for p in ParameterGrid(param_grid):\n",
    "    print(p)\n",
    "    gbrt = GradientBoostingRegressor(loss='ls',n_estimators=200,learning_rate=p['learning_rate'],max_depth=p['max_depth'], random_state=0)\n",
    "    gbrt.fit(Xtrain,ytrain)\n",
    "    ytr_pred=gbrt.predict(Xtrain)\n",
    "    yval_pred=gbrt.predict(Xvalid)\n",
    "    result={}\n",
    "    result['msetr']=mean_squared_error(ytrain,ytr_pred)\n",
    "    result['mse']=mean_squared_error(yvalid,yval_pred)\n",
    "    result['learning_rate']=p['learning_rate']\n",
    "    result['max_depth']=p['max_depth']\n",
    "    result['max_features']=p['max_features']\n",
    "    ds.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>mse</th>\n",
       "      <th>msetr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.088628</td>\n",
       "      <td>0.088139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.088628</td>\n",
       "      <td>0.088139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.104511</td>\n",
       "      <td>0.108578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.104511</td>\n",
       "      <td>0.108578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.119556</td>\n",
       "      <td>0.128064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.119556</td>\n",
       "      <td>0.128064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.123964</td>\n",
       "      <td>0.133281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.123964</td>\n",
       "      <td>0.133281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.130754</td>\n",
       "      <td>0.140747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.130754</td>\n",
       "      <td>0.140747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.138638</td>\n",
       "      <td>0.150378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.138638</td>\n",
       "      <td>0.150378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  max_depth  max_features       mse     msetr\n",
       "4            0.10          3           0.2  0.088628  0.088139\n",
       "5            0.10          3           0.6  0.088628  0.088139\n",
       "2            0.10          2           0.2  0.104511  0.108578\n",
       "3            0.10          2           0.6  0.104511  0.108578\n",
       "0            0.10          1           0.2  0.119556  0.128064\n",
       "1            0.10          1           0.6  0.119556  0.128064\n",
       "10           0.01          3           0.2  0.123964  0.133281\n",
       "11           0.01          3           0.6  0.123964  0.133281\n",
       "8            0.01          2           0.2  0.130754  0.140747\n",
       "9            0.01          2           0.6  0.130754  0.140747\n",
       "6            0.01          1           0.2  0.138638  0.150378\n",
       "7            0.01          1           0.6  0.138638  0.150378"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsdf = pd.DataFrame.from_records(ds)\n",
    "dsdf.sort_values('mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predict on the test data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now predict the sales on the test data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best **GBRT** estimator and fit again on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=0.2, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "gbrt1= GradientBoostingRegressor(loss='ls',n_estimators=200,learning_rate=0.1,max_depth=3,max_features=0.2, random_state=0)\n",
    "gbrt1.fit(Xtr,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Column transformer defined earlier to transform the test dataset and get it in the correct format. Follow the same process that we did for Xtrain and Xvalid.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ct1= ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "testct=ct1.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sales on the test data-set (**test_df**) and store the result in a new column **forecast_gb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.58946443, 8.75156786, 8.85297106, ..., 8.55878161, 8.68105497,\n",
       "       8.64975051])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "forecast_gb=gbrt1.predict(testct)\n",
    "forecast_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now write a function to plot the forecasts vs the actual sales for the training and the test data combined. To do that, we first predict the sales on the **trdf** and **vadf**, concatenate the two dataframes together, add back the actual sales column **(train_resp)** and concatenate it again with **test_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_plot(model, store, Xtrain, Xvalid):\n",
    "    trdf['forecast'] = model.predict(Xtrain)\n",
    "    vadf['forecast'] = model.predict(Xvalid)\n",
    "    train_df = pd.concat([vadf,trdf])\n",
    "    train_df['Actual'] = train_resp\n",
    "    all_data = pd.concat([test_df, train_df])\n",
    "    \n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(pd.to_datetime(all_data[all_data['Store']==store]['Date']),all_data[all_data['Store']==store]['forecast'])\n",
    "    plt.plot(pd.to_datetime(all_data[all_data['Store']==store]['Date']),all_data[all_data['Store']==store]['Actual'])\n",
    "    plt.title('Forecast')\n",
    "    plt.ylabel('Log Sales')\n",
    "    plt.legend(['forecast', 'actual'], loc='best')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the actual vs forecasted sales for Store 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit Kumar Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Ankit Kumar Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Ankit Kumar Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "C:\\Users\\Ankit Kumar Pandey\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "forecast_plot(model=gbrt1,store='Store 1',Xtrain=Xtr,Xvalid=Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3. A Multi-Layer Perceptron Model\n",
    "\n",
    "This is based on the 3rd prize winning entry, whose authors wrote a [paper](https://arxiv.org/pdf/1604.06737.pdf) afterwords.\n",
    "\n",
    "What we are first going to do is to reduce the cardinality dimensionality of our categoricals by using **embeddings**. \n",
    "\n",
    "### What are embeddings?\n",
    "An embedding is a mapping of discrete - categorical - variable to a vector of continuous variables. Neural Network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.\n",
    "This technique is often used not only in recommender systems(via matrix factorization), but also in NLP models such as `word2vec`.\n",
    "\n",
    "In our problem, consider the variable `store` as an example. This is a categorical predictor and we usually **one-hot encode** this - a single store would be of length 1115 bit-vector with one bit flipped on.\n",
    "\n",
    "### Problems with One-hot encoding\n",
    "1. The 1115 stores will have some commonalities, but the one-hot encoding doesn't represent this. The dot-product(cosine similarity) of any 2 one-hot encoded stores will be 0\n",
    "2. The store variable has high cardinality (1115 unique categories) - in this case, the dimensionality of the transformed variable becomes unmanageable\n",
    "\n",
    "Using embeddings for `store` will enable us to learn the store 'personalities', which can then be used later in other models for sales predictions, or even for other tasks.\n",
    "\n",
    "### Training an embedding\n",
    "\n",
    "- Normally you would do a linear or MLP regression with sales as the target, and both continuous and categorical features\n",
    "- We need to replace the 1-hot encoded categorical features by \"lower-width\" embedding features.\n",
    "- This is equivalent to considering a neural network with the output of an additional **Embedding Layer** concatenated in\n",
    "- The Embedding Layer is simply a Linear Regression\n",
    "\n",
    "![](./images/embmlp.png)\n",
    "\n",
    "Here we divide the cardinality by 2 and add 1 to get the no. of embedding features (this is a heuristic). If the cardinality is high, we clamp the size of the latent space down at 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompetitionDistance_missing': (2, 2),\n",
       " 'CloudCover_missing': (2, 2),\n",
       " 'Store': (1115, 50),\n",
       " 'DayOfWeek': (7, 4),\n",
       " 'Year': (3, 2),\n",
       " 'Month': (12, 7),\n",
       " 'Day': (31, 16),\n",
       " 'StateHoliday': (2, 2),\n",
       " 'CompetitionMonthsOpen': (25, 13),\n",
       " 'Promo2Weeks': (26, 14),\n",
       " 'StoreType': (4, 3),\n",
       " 'Assortment': (3, 2),\n",
       " 'CompetitionOpenSinceYear': (23, 12),\n",
       " 'Promo2SinceYear': (8, 5),\n",
       " 'State': (12, 7),\n",
       " 'Week': (52, 27),\n",
       " 'Events': (22, 12),\n",
       " 'Promo_fw': (6, 4),\n",
       " 'Promo_bw': (6, 4),\n",
       " 'StateHoliday_fw': (8, 5),\n",
       " 'StateHoliday_bw': (8, 5),\n",
       " 'SchoolHoliday_fw': (8, 5),\n",
       " 'SchoolHoliday_bw': (8, 5),\n",
       " 'Promo': (2, 2),\n",
       " 'SchoolHoliday': (2, 2)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards={}\n",
    "for k in nacols_cat:\n",
    "    cards[k] = (2,2)\n",
    "for k in cat_vars :\n",
    "    embed_sz_base = trdf[k].unique().size//2 + 1\n",
    "    embed_sz = (embed_sz_base <=50)*embed_sz_base + 50*((embed_sz_base > 50))\n",
    "    cards[k] = (trdf[k].unique().size, embed_sz)\n",
    "cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building the model architecture using the Keras Functional API\n",
    "We have to be careful (very book-keepy) in constructing a model in Keras. We use the Keras Functional API as opposed to the Sequential API. The architecture can be summarized in the below steps:\n",
    "1. 25 input layers for categorical variables which feed into an Embedding Layer and then into a Reshape Layer\n",
    "2. 14 input layers for continuous variables\n",
    "3. Concatenating the 25 Reshape layers and the 14 input layers\n",
    "4. Connecting the concatenated layer to a 1000 unit Dense Layer, which is connected to a 500 unit Dense Layer, which is then connected to the Output - a single unit Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras) (1.2.1)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras) (5.1.1)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/54/5f/e1b2d83b808f978f51b7ce109315154da3a3d4151aa59686002681f2e109/tensorflow-2.0.0-cp37-cp37m-win_amd64.whl (48.1MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/14/ab1501cfff78b88d7368659b227c603d7599dd25226ff682c71334e78aed/grpcio-1.26.0-cp37-cp37m-win_amd64.whl (1.8MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/30/c6/286db43e2d0d4b89d328a222365c7a253a99a24067812253f0d4f8eb0f1c/protobuf-3.11.2-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Requirement already satisfied: h5py in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.15.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.0.1)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ankit kumar pandey\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.6.16)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Building wheels for collected packages: absl-py, termcolor, gast, opt-einsum\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ankit Kumar Pandey\\AppData\\Local\\pip\\Cache\\wheels\\8e\\28\\49\\fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ankit Kumar Pandey\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ankit Kumar Pandey\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for opt-einsum (setup.py): started\n",
      "  Building wheel for opt-einsum (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ankit Kumar Pandey\\AppData\\Local\\pip\\Cache\\wheels\\2c\\b1\\94\\43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "Successfully built absl-py termcolor gast opt-einsum\n",
      "Installing collected packages: absl-py, grpcio, termcolor, gast, google-pasta, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, google-auth-oauthlib, markdown, protobuf, tensorboard, tensorflow-estimator, opt-einsum, astor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 gast-0.2.2 google-auth-1.10.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.26.0 markdown-3.1.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.2 pyasn1-0.4.8 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def build_keras_model():\n",
    "    input_cat = []\n",
    "    output_embeddings = []\n",
    "    for k in nacols_cat+cat_vars:\n",
    "        print('{}_embedding'.format(k))\n",
    "        input_1d = Input(shape=(1,))\n",
    "        output_1d = Embedding(cards[k][0], cards[k][1], name='{}_embedding'.format(k))(input_1d)\n",
    "        output = Reshape(target_shape=(cards[k][1],))(output_1d)\n",
    "        input_cat.append(input_1d)\n",
    "        output_embeddings.append(output)\n",
    "\n",
    "    main_input = Input(shape=(len(cont_vars),), name='main_input')\n",
    "    output_model = Concatenate()([main_input, *output_embeddings])\n",
    "    output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "    output_model = Activation('relu')(output_model)\n",
    "    output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "    output_model = Activation('relu')(output_model)\n",
    "    output_model = Dense(1)(output_model)\n",
    "    kmodel = KerasModel(\n",
    "        inputs=[*input_cat, main_input], \n",
    "        outputs=output_model\n",
    ")\n",
    "    kmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return kmodel\n",
    "\n",
    "def fitmodel(kmodel, Xtr, ytr, Xval, yval, epochs, bs):\n",
    "    h = kmodel.fit(Xtr, ytr, validation_data=(Xval, yval),\n",
    "                       epochs=epochs, batch_size=bs)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data input needs to match our construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat_trains=[]\n",
    "list_cat_valids=[]\n",
    "catlen=len(nacols_cat+cat_vars)\n",
    "for i in range(catlen):\n",
    "    list_cat_trains.append(Xtrain[:,i])\n",
    "    list_cat_valids.append(Xvalid[:,i])\n",
    "cont_train=Xtrain[:,catlen:]\n",
    "cont_valid=Xvalid[:,catlen:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run the model for 10 epochs and a batch-size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'validation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-cf07bded817e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfitmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgbrt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myvalid\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-163-ed7d3ff86e47>\u001b[0m in \u001b[0;36mfitmodel\u001b[1;34m(kmodel, Xtr, ytr, Xval, yval, epochs, bs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfitmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     h = kmodel.fit(Xtr, ytr, validation_data=(Xval, yval),\n\u001b[1;32m---> 34\u001b[1;33m                        epochs=epochs, batch_size=bs)\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'validation_data'"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "fitmodel(kmodel=gbrt1, Xtr=Xtrain,ytr=ytrain, Xval=Xval,yval=yvalid , epochs=10,bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training vs validation loss\n",
    "def plot_model_history(model_history):\n",
    "    # summarize history for loss\n",
    "    plt.plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    plt.plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylim([0,0.3])\n",
    "    plt.legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the Categorical Embedding model perform as opposed to the GBRT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the forecasts on the test data-set and then compare the actuals vs forecast for the entire data (train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the actuals vs forecasts on the entire data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
